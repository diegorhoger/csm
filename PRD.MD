# ðŸ§  Summary of Functionalities for Your Stoic Mentor System (with CSM)

## 1. Conversational Voice Interface

- Always-on voice listening with wake phrase or open-mic toggle.
- Real-time speech-to-text (Whisper): Transcribes user's voice.
- Context-aware streaming response from GPT-4 (or Claude/Solar): Emulates Marcus Aurelius, Seneca, or Epictetus.
- Streaming TTS response using ElevenLabs or Bark with natural, expressive voice synthesis.
- Interrupt detection: User can speak over the AI to cut it off (mimics natural turn-taking).

## 2. Mentor Personality System

Each mentor has:

- Unique system prompt/personality style
- Custom voice profile (TTS voice ID)
- Mentor-specific tone control (calm, motivational, challenging)
- Future: dynamic emotion/mood adjustment (e.g., gentler if user is sad)

## 3. Minimalist Interface Like Sesame Demo

No chat bubbles, no keyboard unless requested.

Simple UI showing:

- Mic level or waveform
- Current speaker (User / Mentor)
- Option to switch mentors (Marcus / Seneca / Epictetus)
- Controls: Start / Stop conversation, optionally Pause.

## 4. Real-Time Feedback Loop

- Streaming Whisper input
- Streaming GPT response with async generator
- Streaming TTS output with interrupt support
- 200ms latency goal: parallelizing transcription â†’ generation â†’ speech

## 5. Session Memory

- Light memory in context window (e.g., last 3 exchanges)
- Local conversation log (per mentor)
- Option to mark reflections for journaling or later review

## 6. Stoic Practice Layer
(Optional for V2 â€” but architect early)

After or during conversations, mentors might suggest:

- Reflection prompts
- Breathing techniques
- Quotes to remember
- Auto-tag journal entries based on themes/emotions

## 7. Local Dev & Backend Plan

For now: all runs locally on your machine

Components:

- `useMicStream` (or vad): listens to audio
- `startWhisperStream`: transcribes
- `startGPTStream`: responds
- `streamTTS`: speaks

Future backend:

- Firebase or Supabase for saving sessions
- DB model: user > mentor > session > message[]

## 8. Expandable Architecture

CSM lets you easily:

- Swap STT, LLM, or TTS provider
- Plug in emotion detection
- Add eye/contact detection (for mobile/AR later)
- Run on-device (edge) with fast models

---

## ðŸ§± Project Directory Structure

```
/src
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ UI/
â”‚   â”‚   â””â”€â”€ WaveformVisualizer.tsx
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ MentorCallUI.tsx
â”‚   â”‚   â””â”€â”€ MentorSwitcher.tsx
â”‚   â””â”€â”€ layout/
â”‚       â””â”€â”€ AppLayout.tsx
â”‚
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useMentorCallEngine.ts        # Core logic: whisper + GPT + TTS
â”‚   â”œâ”€â”€ useMicStream.ts               # Gets microphone input
â”‚   â”œâ”€â”€ useVoiceActivityDetection.ts  # Detects user speaking (for interruption)
â”‚   â”œâ”€â”€ useWakeWord.ts                # (Optional) Trigger phrase "Hey Marcus"
â”‚   â””â”€â”€ useSessionRecorder.ts         # Record/log conversations
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ whisper.ts                    # Transcription service wrapper
â”‚   â”œâ”€â”€ openai.ts                     # GPT streaming wrapper
â”‚   â”œâ”€â”€ tts.ts                        # Text-to-speech streaming
â”‚   â”œâ”€â”€ mentors.ts                    # Mentor personalities (prompts + voices)
â”‚   â””â”€â”€ emotion.ts                    # (Optional) Sentiment/emotion detection
â”‚
â”œâ”€â”€ state/
â”‚   â””â”€â”€ sessionStore.ts               # Store for conversation state (Zustand, Jotai, etc.)
â”‚
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ index.tsx                     # Home / main conversation screen
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ throttle.ts
â”‚   â”œâ”€â”€ debounce.ts
â”‚   â””â”€â”€ audioUtils.ts
â”‚
â”œâ”€â”€ constants/
â”‚   â””â”€â”€ app.ts                        # App-wide constants (latency target, etc.)
â”‚
â”œâ”€â”€ types/
â”‚   â””â”€â”€ index.ts                      # Shared types/interfaces
â”‚
â””â”€â”€ csm/                              # Sesame CSM integration
    â”œâ”€â”€ graph.ts                      # Conversation state machine logic
    â””â”€â”€ hooks.ts                      # CSM-powered React helpers
```

## ðŸ’¡ Key Concepts

### useMentorCallEngine.ts

Central orchestrator:

- Handles mic input â†’ Whisper
- Feeds into GPT stream (based on selected mentor's prompt)
- Sends TTS chunks
- Switches between speaking/listening states
- Integrates CSM transitions

### services/mentors.ts

Encodes Stoic mentor personalities:

```typescript
export const mentors = {
  marcus: {
    name: 'Marcus Aurelius',
    prompt: 'You are Marcus Aurelius. Speak calmly and with quiet strength...',
    voiceId: 'marcus-v1',
    style: 'calm'
  },
  seneca: {
    name: 'Seneca',
    prompt: 'You are Seneca. Speak with eloquence and motivation...',
    voiceId: 'seneca-v1',
    style: 'motivational'
  },
  epictetus: {
    name: 'Epictetus',
    prompt: 'You are Epictetus. Speak bluntly and challenge assumptions...',
    voiceId: 'epictetus-v1',
    style: 'firm'
  }
};
```

### components/core/MentorCallUI.tsx

Main interactive conversation interface:

- Big mic button (start/stop)
- Visual feedback (waveform, transcription)
- Optional log pane (transcript + mentor response)

### state/sessionStore.ts

Session memory state:

```typescript
interface Message {
  role: 'user' | 'mentor';
  content: string;
  timestamp: number;
}

interface SessionState {
  currentMentor: MentorKey;
  history: Message[];
  isSpeaking: boolean;
  isListening: boolean;
}
```

## âœ… Optional Modules (Future)

- `/tasks/stoicPractices.ts` â€“ Assign suggested Stoic reflections
- `/api/feedback.ts` â€“ Send logs or feedback to backend
- `/mobile/` â€“ Build React Native interface using same logic